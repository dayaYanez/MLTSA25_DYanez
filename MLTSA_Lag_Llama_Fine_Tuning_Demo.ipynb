{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dayaYanez/MLTSA25_DYanez/blob/main/MLTSA_Lag_Llama_Fine_Tuning_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11bde39e-dfcb-47df-b15f-4abe83cf9431",
      "metadata": {
        "id": "11bde39e-dfcb-47df-b15f-4abe83cf9431"
      },
      "source": [
        "# Lag-Llama Fine Tuning MLTSA\n",
        "\n",
        "## Prepare the repository\n",
        "\n",
        "We first clone and install the required packages from the [GitHub repository](https://github.com/time-series-foundation-models/lag-llama/) that has the Lag-Llama architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f314d613-aac8-499f-a632-3a9062e1293e",
      "metadata": {
        "id": "f314d613-aac8-499f-a632-3a9062e1293e",
        "outputId": "b311bd85-7d65-4e23-a3e7-109b3c13a4c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lag-llama'...\n",
            "remote: Enumerating objects: 505, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 505 (delta 154), reused 109 (delta 109), pack-reused 312 (from 3)\u001b[K\n",
            "Receiving objects: 100% (505/505), 283.34 KiB | 9.77 MiB/s, done.\n",
            "Resolving deltas: 100% (252/252), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b update-gluonts https://github.com/time-series-foundation-models/lag-llama/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9b3a4589-1ff7-46b3-8ad3-b2c2bc998183",
      "metadata": {
        "id": "9b3a4589-1ff7-46b3-8ad3-b2c2bc998183",
        "outputId": "0f414544-e0ce-4886-873d-c86e99e585ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lag-llama\n"
          ]
        }
      ],
      "source": [
        "cd /content/lag-llama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall jax jaxlib tensorboard -y"
      ],
      "metadata": {
        "id": "dMPlOs3HhyYO",
        "outputId": "b7158ba9-6573-4442-a21a-130a8077bb44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dMPlOs3HhyYO",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jax 0.5.2\n",
            "Uninstalling jax-0.5.2:\n",
            "  Successfully uninstalled jax-0.5.2\n",
            "Found existing installation: jaxlib 0.5.1\n",
            "Uninstalling jaxlib-0.5.1:\n",
            "  Successfully uninstalled jaxlib-0.5.1\n",
            "Found existing installation: tensorboard 2.18.0\n",
            "Uninstalling tensorboard-2.18.0:\n",
            "  Successfully uninstalled tensorboard-2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1cb4157e-a745-4938-b8c8-06eeeb57eeaa",
      "metadata": {
        "id": "1cb4157e-a745-4938-b8c8-06eeeb57eeaa",
        "outputId": "bb7abc04-c3b4-44ed-be17-c64091458330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m484.2/484.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -r requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bdcf8e7-b6f7-4e85-be59-cc6258e6c3b2",
      "metadata": {
        "id": "9bdcf8e7-b6f7-4e85-be59-cc6258e6c3b2"
      },
      "source": [
        "We then download our pretrained model weights from [HuggingFace](https://huggingface.co/time-series-foundation-models/Lag-Llama) ğŸ¤—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "80c5f8e7-3687-4f4c-8e42-2d84882845e1",
      "metadata": {
        "id": "80c5f8e7-3687-4f4c-8e42-2d84882845e1",
        "outputId": "05f404e3-a641-4e06-c8fc-75b386cd1e95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'lag-llama.ckpt' to '/content/lag-llama/.cache/huggingface/download/59Iq1KnnyJzBevZl6u7vR_lVuAs=.b5a5c4b8a0cfe9b81bdac35ed5d88b5033cd119b5206c28e9cd67c4b45fb2c96.incomplete'\n",
            "lag-llama.ckpt: 100% 29.5M/29.5M [00:00<00:00, 192MB/s]\n",
            "Download complete. Moving file to /content/lag-llama/lag-llama.ckpt\n",
            "/content/lag-llama/lag-llama.ckpt\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir /content/lag-llama"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c2ea0b9-4b10-4a8a-a973-f8a53c4993e3",
      "metadata": {
        "id": "7c2ea0b9-4b10-4a8a-a973-f8a53c4993e3"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We import the required packages and the lag llama estimator object which we can use to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12626ee4-7175-4854-8701-7cc6a4f514d8",
      "metadata": {
        "id": "12626ee4-7175-4854-8701-7cc6a4f514d8",
        "outputId": "b0fd33e8-aaf8-428b-a3ab-92fd87cf8ed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b6a62eef4d70>:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "from itertools import islice\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from tqdm.autonotebook import tqdm\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "import torch\n",
        "\n",
        "from lag_llama.gluon.estimator import LagLlamaEstimator\n",
        "\n",
        "import sys\n",
        "from types import ModuleType\n",
        "\n",
        "from gluonts.evaluation import make_evaluation_predictions, MultivariateEvaluator\n",
        "from gluonts.evaluation import Evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy module hierarchy\n",
        "def create_dummy_module(module_path):\n",
        "    \"\"\"\n",
        "    Create a dummy module hierarchy for the given path.\n",
        "    Returns the leaf module.\n",
        "    \"\"\"\n",
        "    parts = module_path.split('.')\n",
        "    current = ''\n",
        "    parent = None\n",
        "\n",
        "    for part in parts:\n",
        "        current = current + '.' + part if current else part\n",
        "        if current not in sys.modules:\n",
        "            module = ModuleType(current)\n",
        "            sys.modules[current] = module\n",
        "            if parent:\n",
        "                setattr(sys.modules[parent], part, module)\n",
        "        parent = current\n",
        "\n",
        "    return sys.modules[module_path]\n",
        "\n",
        "# Create the dummy gluonts module hierarchy\n",
        "gluonts_module = create_dummy_module('gluonts.torch.modules.loss')\n",
        "\n",
        "# Create dummy classes for the specific loss functions\n",
        "class DistributionLoss:\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return 0.0\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return lambda *args, **kwargs: None\n",
        "\n",
        "class NegativeLogLikelihood:\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return 0.0\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return lambda *args, **kwargs: None\n",
        "\n",
        "# Add the specific classes to the module\n",
        "gluonts_module.DistributionLoss = DistributionLoss\n",
        "gluonts_module.NegativeLogLikelihood = NegativeLogLikelihood"
      ],
      "metadata": {
        "id": "GZmoFdvAevcA"
      },
      "id": "GZmoFdvAevcA",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "19313809-514c-4eca-8bd2-7de76393bda5",
      "metadata": {
        "id": "19313809-514c-4eca-8bd2-7de76393bda5"
      },
      "source": [
        "We create a function for Lag-Llama inference that we can reuse. This function returns the predictions for the given prediction horizon. The forecast will be of shape (num_samples, prediction_length), where `num_samples` is the number of samples sampled from the predicted probability distribution for each timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e4c2405a-be1c-4f7d-b598-580c974a68fb",
      "metadata": {
        "id": "e4c2405a-be1c-4f7d-b598-580c974a68fb"
      },
      "outputs": [],
      "source": [
        "def get_lag_llama_predictions(dataset, prediction_length, context_length=32,\n",
        "              num_samples=20, device=\"cuda\", batch_size=64, nonnegative_pred_samples=True):\n",
        "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device, weights_only=False) # modified\n",
        "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "    estimator = LagLlamaEstimator(\n",
        "        ckpt_path=\"lag-llama.ckpt\",\n",
        "        prediction_length=prediction_length,\n",
        "        context_length=context_length,\n",
        "\n",
        "        # estimator args\n",
        "        input_size=estimator_args[\"input_size\"],\n",
        "        n_layer=estimator_args[\"n_layer\"],\n",
        "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "        n_head=estimator_args[\"n_head\"],\n",
        "        scaling=estimator_args[\"scaling\"],\n",
        "        time_feat=estimator_args[\"time_feat\"],\n",
        "\n",
        "        nonnegative_pred_samples=nonnegative_pred_samples,\n",
        "\n",
        "        # linear positional encoding scaling\n",
        "        rope_scaling={\n",
        "            \"type\": \"linear\",\n",
        "            \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
        "        },\n",
        "\n",
        "        batch_size=batch_size,\n",
        "        num_parallel_samples=num_samples,\n",
        "    )\n",
        "\n",
        "    lightning_module = estimator.create_lightning_module()\n",
        "    transformation = estimator.create_transformation()\n",
        "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
        "\n",
        "    forecast_it, ts_it = make_evaluation_predictions(\n",
        "        dataset=dataset,\n",
        "        predictor=predictor,\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "    forecasts = list(tqdm(forecast_it, total=len(dataset), desc=\"Forecasting batches\"))\n",
        "    tss = list(tqdm(ts_it, total=len(dataset), desc=\"Ground truth\"))\n",
        "\n",
        "    return forecasts, tss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b5531e-8ad3-4465-b400-33465eff0340",
      "metadata": {
        "id": "06b5531e-8ad3-4465-b400-33465eff0340"
      },
      "source": [
        "# Zero Shot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data : `M4-Weekly`"
      ],
      "metadata": {
        "id": "6AOSBtQ_SC87"
      },
      "id": "6AOSBtQ_SC87"
    },
    {
      "cell_type": "code",
      "source": [
        "from gluonts.dataset.repository import get_dataset\n",
        "dataset = get_dataset(\"m4_weekly\")\n",
        "train_dataset = dataset.train\n",
        "test_dataset = dataset.test"
      ],
      "metadata": {
        "id": "3ghQNSKAJsR7"
      },
      "id": "3ghQNSKAJsR7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"dataset length: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hjBp6M5YO31",
        "outputId": "d126e5a7-8114-461c-b8c3-e9199f4d1a66"
      },
      "id": "2hjBp6M5YO31",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset length: 359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_length = dataset.metadata.prediction_length\n",
        "context_length = prediction_length * 3\n",
        "num_samples = 20\n",
        "device = \"cuda\"\n",
        "print(f\"prediction_length {prediction_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kK-l2G7kJp2",
        "outputId": "794ae7a6-6db0-430c-813a-0f25c4710642"
      },
      "id": "0kK-l2G7kJp2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction_length 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NEW DATASET"
      ],
      "metadata": {
        "id": "GSVFbv2mRXhz"
      },
      "id": "GSVFbv2mRXhz"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!git clone https://github.com/nlandolfi/acc2022treelinearcascades_stocks.git"
      ],
      "metadata": {
        "id": "3QBYhceV8stm",
        "cellView": "form"
      },
      "id": "3QBYhceV8stm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "files = glob.glob(\"acc2022treelinearcascades_stocks/data/*\")\n",
        "files"
      ],
      "metadata": {
        "id": "t4Llh1Jy9PWw",
        "cellView": "form"
      },
      "id": "t4Llh1Jy9PWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "pd.read_csv(files[0], index_col=0)"
      ],
      "metadata": {
        "id": "tM5-aS8f91Xm",
        "cellView": "form",
        "collapsed": true
      },
      "id": "tM5-aS8f91Xm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "data = np.zeros((len(files)-1,  pd.read_csv(files[0], index_col=0).shape[0])) #prep data array for all files"
      ],
      "metadata": {
        "id": "CqoMvFDC9ZSV",
        "cellView": "form"
      },
      "id": "CqoMvFDC9ZSV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "j = 0\n",
        "for i,f in enumerate(files):\n",
        "  _ = pd.read_csv(f, index_col=0)\n",
        "  if len(_) < data.shape[1]:\n",
        "    continue\n",
        "\n",
        "  data[j, :len(_[' Volume'])] = _[' Volume'].values\n",
        "  j += 1\n",
        "data.shape"
      ],
      "metadata": {
        "id": "ztGD11Ap93Sf",
        "cellView": "form",
        "collapsed": true
      },
      "id": "ztGD11Ap93Sf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "x = pd.to_datetime(_.index)\n",
        "fig, ax = plt.subplots(3,3, figsize=(20,10), sharex=True)\n",
        "for i in range(9):\n",
        "  ax[i%3, i//3].plot(x, data[i])\n",
        "  if i%3 > 1:\n",
        "    ax[i%3, i//3].set_xticklabels(labels=ax[i%3, i//3].get_xticklabels(), rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xs78Gx7PbPre"
      },
      "id": "xs78Gx7PbPre",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "x.diff()"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "70RJq_UIdLqo"
      },
      "id": "70RJq_UIdLqo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df = pd.DataFrame(data=data).T\n",
        "df.index= pd.to_datetime(x, format=\"%m/%d/%y\")\n",
        "df_resampled = df.resample('2d').mean()#.fillna(df.mean())\n",
        "df_resampled = df_resampled.interpolate(limit_direction='both')\n",
        "df_resampled.index.diff().unique()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RoQ0_tdzdTJS"
      },
      "id": "RoQ0_tdzdTJS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df_resampled.shape"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ax1eI3KCeJFL"
      },
      "id": "Ax1eI3KCeJFL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "x = df_resampled.index\n",
        "fig, ax = plt.subplots(3,3, figsize=(20,10), sharex=True)\n",
        "for i in range(9):\n",
        "  ax[i%3, i//3].plot(x, df_resampled.iloc[:,i])\n",
        "  if i%3 > 1:\n",
        "    ax[i%3, i//3].set_xticklabels(labels=ax[i%3, i//3].get_xticklabels(), rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5UPRkShPdqJ-"
      },
      "id": "5UPRkShPdqJ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# prompt: lowpass filter the imput data\n",
        "from scipy.signal import butter, lfilter, filtfilt\n",
        "\n",
        "\n",
        "def butter_lowpass(cutoff, fs, order=5):\n",
        "    \"\"\"\n",
        "    Design a digital Butterworth low-pass filter.\n",
        "\n",
        "    The Butterworth filter is characterized by a maximally flat magnitude response\n",
        "    in the passband and a monotonic roll-off in the stopband.  This function\n",
        "    calculates the filter coefficients for a low-pass Butterworth filter.\n",
        "\n",
        "    Args:\n",
        "        cutoff (float): The cutoff frequency of the filter in Hz.  This is the\n",
        "            frequency at which the filter's gain is reduced by 3 dB (half-power).\n",
        "        fs (float): The sampling rate of the signal to be filtered, in Hz.\n",
        "        order (int, optional): The order of the filter.  Higher order filters\n",
        "            provide a steeper roll-off but also increase the filter's complexity\n",
        "            and introduce more phase delay.  The default is 5.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of two NumPy arrays, (b, a).\n",
        "            - b (ndarray): The numerator coefficients of the filter's transfer function.\n",
        "            - a (ndarray): The denominator coefficients of the filter's transfer function.\n",
        "\n",
        "    The filter's transfer function is defined as:\n",
        "    âˆ£H(jÏ‰)âˆ£^2 = 1 / (1 + (Ï‰/Ï‰c)^(2n))\n",
        "\n",
        "    Where:\n",
        "    - Ï‰ is the angular frequency (2 * pi * frequency)\n",
        "    - Ï‰c is the cutoff angular frequency (2 * pi * cutoff)\n",
        "    - n is the filter order\n",
        "    \"\"\"\n",
        "\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    return b, a\n",
        "\n",
        "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
        "    \"\"\"\n",
        "    Apply a digital Butterworth low-pass filter to input data.\n",
        "\n",
        "    This function designs and applies a Butterworth low-pass filter to the given\n",
        "    input data using forward-backward filtering to minimize phase distortion.\n",
        "\n",
        "    Args:\n",
        "        data (ndarray): The input data to be filtered.  It should be a 1-D NumPy array.\n",
        "        cutoff (float): The cutoff frequency of the filter in Hz.\n",
        "            Frequencies above this will be attenuated.\n",
        "        fs (float): The sampling rate of the input data, in Hz.\n",
        "        order (int, optional): The order of the filter. Higher order filters\n",
        "            provide a steeper roll-off but can increase complexity. The default is 5.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The filtered data, with the same shape as the input data.\n",
        "\n",
        "    \"\"\"\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "fs = 1/48  # Replace with your actual sampling frequency this is in hours\n",
        "\n",
        "# Choose cutoff frequency (adjust based on desired filtering characteristics)\n",
        "cutoff = 1/48/50\n",
        "\n",
        "# Apply filter to each time series in 'data'\n",
        "filtered_data = np.zeros_like(df_resampled.T.values)\n",
        "for i in range(filtered_data.shape[0]):\n",
        "    filtered_data[i, :] = butter_lowpass_filter(df_resampled.iloc[:,i], cutoff, fs, order=5)\n",
        "\n",
        "# Now 'filtered_data' contains the low-pass filtered time series data\n",
        "# You can proceed to plot or further analyze the filtered data\n",
        "\n",
        "fig, ax = plt.subplots(3,3, figsize=(20,10), sharex=True)\n",
        "for i in range(9):\n",
        "  ax[i%3, i//3].plot(x, df_resampled.iloc[:,i])\n",
        "  ax[i%3, i//3].plot(x, filtered_data[i], 'r')\n",
        "  if i%3 > 1:\n",
        "    ax[i%3, i//3].set_xticklabels(labels=ax[i%3, i//3].get_xticklabels(), rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z_fyLh2D-zYg",
        "cellView": "form"
      },
      "id": "z_fyLh2D-zYg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df_filtered = pd.DataFrame(data=filtered_data).T\n",
        "df_filtered.index = x\n",
        "df_filtered.index.diff().unique(),\n"
      ],
      "metadata": {
        "id": "4P1XCnATB4eA",
        "cellView": "form"
      },
      "id": "4P1XCnATB4eA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df_filtered"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mgRQtwxSSOqZ"
      },
      "id": "mgRQtwxSSOqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#we are not going to use it but useful cause it may be inputts in gluonts: how do i make it a single column df?\n",
        "df_filtered_long = df_filtered.reset_index()\n",
        "df_filtered_long = df_filtered_long.melt(id_vars=\"Date\")\n",
        "df_filtered_long.set_index(\"Date\", inplace=True)\n",
        "df_filtered_long.index = pd.to_datetime(df_filtered_long.index)\n",
        "df_filtered_long"
      ],
      "metadata": {
        "id": "ivSYCZKG-ebZ",
        "cellView": "form"
      },
      "id": "ivSYCZKG-ebZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "ax = df_filtered.plot(legend=False)\n",
        "ax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90);\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wdRMEv9ggt3E"
      },
      "id": "wdRMEv9ggt3E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df_filtered.isna().sum().sum()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bq7XvQ5lRLS5"
      },
      "id": "bq7XvQ5lRLS5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#import into gluonts\n",
        "from gluonts.dataset.field_names import FieldName\n",
        "from gluonts.dataset.common import ListDataset\n",
        "\n",
        "start_date = df_filtered.index[0]\n",
        "train_data = []\n",
        "test_data = []\n",
        "for i in range(len(df_filtered.columns)):\n",
        "    # Create synthetic time series with some dependencies\n",
        "    target = df_filtered.iloc[:,i]\n",
        "\n",
        "    train_target = target[:-prediction_length]\n",
        "    test_target = target\n",
        "\n",
        "    train_data.append({\n",
        "        FieldName.TARGET: train_target,\n",
        "        FieldName.START: start_date,\n",
        "        FieldName.ITEM_ID: f\"series_{i}\"\n",
        "    })\n",
        "    test_data.append({\n",
        "        FieldName.TARGET: test_target,\n",
        "        FieldName.START: start_date,\n",
        "        FieldName.ITEM_ID: f\"series_{i}\"\n",
        "    })\n",
        "\n",
        "# Convert to GluonTS ListDataset\n",
        "train_dataset = ListDataset(train_data, freq=\"2d\")\n",
        "test_dataset = ListDataset(test_data, freq=\"2d\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MEqL4NM8eIvX"
      },
      "id": "MEqL4NM8eIvX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## predict"
      ],
      "metadata": {
        "id": "Cy3Fpg_RSMLl"
      },
      "id": "Cy3Fpg_RSMLl"
    },
    {
      "cell_type": "code",
      "source": [
        "list(train_dataset)[0]"
      ],
      "metadata": {
        "id": "s6JX9doIXSZb"
      },
      "id": "s6JX9doIXSZb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(train_dataset)[0][\"target\"])\n"
      ],
      "metadata": {
        "id": "fJ1F6y5qXHLr"
      },
      "id": "fJ1F6y5qXHLr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecasts, tss = get_lag_llama_predictions(\n",
        "    test_dataset,\n",
        "    prediction_length=prediction_length,\n",
        "    num_samples=num_samples,\n",
        "    context_length=context_length,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "fC2IODxWkJmA"
      },
      "id": "fC2IODxWkJmA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c8783db-3a6b-4eb6-b2cd-05e25614cbe2",
      "metadata": {
        "id": "6c8783db-3a6b-4eb6-b2cd-05e25614cbe2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 15))\n",
        "date_formater = mdates.DateFormatter('%b, %d')\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "# Iterate through the first 9 series, and plot the predicted samples\n",
        "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
        "    ax = plt.subplot(3, 3, idx+1)\n",
        "\n",
        "    plt.plot(ts[-25 * prediction_length:].to_timestamp(), label=\"target\", )\n",
        "    forecast.plot( color='g')\n",
        "    plt.xticks(rotation=60)\n",
        "    ax.xaxis.set_major_formatter(date_formater)\n",
        "    ax.set_title(forecast.item_id)\n",
        "\n",
        "plt.gcf().tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7a9f417-784e-4425-bccf-7fdb10a0df56",
      "metadata": {
        "id": "a7a9f417-784e-4425-bccf-7fdb10a0df56"
      },
      "source": [
        "# Fine-tuning\n",
        "\n",
        "Let us fine-tune the Lag-Llama base model with a few data-specific changes. Feel Free to change the hyperparameters below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de228599-5404-4b62-87cd-4b9573583981",
      "metadata": {
        "id": "de228599-5404-4b62-87cd-4b9573583981"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load(\"lag-llama.ckpt\", map_location=device, weights_only=False)\n",
        "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "estimator = LagLlamaEstimator(\n",
        "        ckpt_path=\"lag-llama.ckpt\",\n",
        "        prediction_length=prediction_length,\n",
        "        context_length=context_length,\n",
        "\n",
        "        # distr_output=\"neg_bin\",\n",
        "        # scaling=\"mean\",\n",
        "        nonnegative_pred_samples=True,\n",
        "        aug_prob=0,\n",
        "        lr=5e-4,\n",
        "        #lr=5e-6,\n",
        "\n",
        "        # estimator args\n",
        "        input_size=estimator_args[\"input_size\"],\n",
        "        n_layer=estimator_args[\"n_layer\"],\n",
        "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "        n_head=estimator_args[\"n_head\"],\n",
        "        time_feat=estimator_args[\"time_feat\"],\n",
        "\n",
        "        # rope_scaling={\n",
        "        #     \"type\": \"linear\",\n",
        "        #     \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
        "        # },\n",
        "\n",
        "        batch_size=64,\n",
        "        num_parallel_samples=num_samples,\n",
        "        trainer_kwargs = {\"max_epochs\": 50,}, # <- lightning trainer arguments\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2b75bd-657d-438b-8596-f97509253c92",
      "metadata": {
        "id": "7b2b75bd-657d-438b-8596-f97509253c92"
      },
      "outputs": [],
      "source": [
        "predictor = estimator.train(train_dataset,\n",
        "    cache_data=True, shuffle_buffer_length=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a88620-5a56-463c-adf7-ef13018da432",
      "metadata": {
        "id": "54a88620-5a56-463c-adf7-ef13018da432"
      },
      "outputs": [],
      "source": [
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "        test_dataset,\n",
        "        predictor=predictor,\n",
        "        num_samples=num_samples\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3413cbf1-7e58-4c16-84a1-4c92d9926908",
      "metadata": {
        "id": "3413cbf1-7e58-4c16-84a1-4c92d9926908"
      },
      "outputs": [],
      "source": [
        "forecasts = list(tqdm(forecast_it, total=len(test_dataset), desc=\"Forecasting batches\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf547aa-4c47-40b5-bf14-0835601a24d7",
      "metadata": {
        "id": "acf547aa-4c47-40b5-bf14-0835601a24d7"
      },
      "outputs": [],
      "source": [
        "tss = list(tqdm(ts_it, total=len(test_dataset), desc=\"Ground truth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a310d3c-961c-412f-983d-039f5e04fbf7",
      "metadata": {
        "id": "6a310d3c-961c-412f-983d-039f5e04fbf7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 15))\n",
        "date_formater = mdates.DateFormatter('%b, %d')\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "# Iterate through the first 9 series, and plot the predicted samples\n",
        "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
        "    ax = plt.subplot(3, 3, idx+1)\n",
        "\n",
        "    plt.plot(ts[-25 * prediction_length:].to_timestamp(), label=\"target\", )\n",
        "    forecast.plot( color='g')\n",
        "    plt.xticks(rotation=60)\n",
        "    ax.xaxis.set_major_formatter(date_formater)\n",
        "    ax.set_title(forecast.item_id)\n",
        "\n",
        "plt.gcf().tight_layout()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9360f22e-7438-4f5b-a20b-6ab83e25d3c4",
      "metadata": {
        "id": "9360f22e-7438-4f5b-a20b-6ab83e25d3c4"
      },
      "outputs": [],
      "source": [
        "evaluator = Evaluator()\n",
        "agg_metrics, ts_metrics = evaluator(iter(tss), iter(forecasts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19be712b-c04d-44a6-8dfa-c151f991f78e",
      "metadata": {
        "id": "19be712b-c04d-44a6-8dfa-c151f991f78e"
      },
      "outputs": [],
      "source": [
        "agg_metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjxezGBnS_5u"
      },
      "id": "bjxezGBnS_5u",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}